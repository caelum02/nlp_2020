{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "258f18fc-e0b2-4a55-8927-51c46077017b",
    "_uuid": "4c26dad8c106fb2eacc6d703993b524774467445"
   },
   "source": [
    "Introduction\n",
    "====\n",
    "**Natural Language Processing ** (NLP) is the task of making computers understand and produce human languages. \n",
    "\n",
    "And it always starts with the **corpus** i.e. *a body of text*. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2762e23e-138d-4086-af46-53aa1d7d0bcd",
    "_uuid": "93f8697704b4f7c9a900bd26c341862d1ef82f05"
   },
   "source": [
    "\n",
    "What is a Corpus?\n",
    "====\n",
    "\n",
    "There are many corpora (*plural of corpus*) available in NLTK, lets start with an English one call the **Brown corpus**.\n",
    "\n",
    "When using a new corpus in NLTK for the first time, downloads the corpus with the `nltk.download()` function, e.g. \n",
    "\n",
    "```python\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bd910c35-dc49-4c07-9441-9d16d175580a",
    "_uuid": "6a663051c176297596bc3174d10a1f8599247621"
   },
   "source": [
    "After its downloaded, you can import it as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /csehome/caelum02/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "2f1d4c2c-ea22-4494-a650-cfddd3aba49c",
    "_uuid": "637fad23e6bdfd6b8188a7c56318c91b6d3227e2"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "121fd272-19fd-498f-a3e9-5295d09a2e16",
    "_uuid": "14c6fc8bb6bdffbf6fb397c9d2c5da53d12c1e69"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words() # Returns a list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "ebf89848-9442-4790-9575-a7f87234eebd",
    "_uuid": "2841227d2eeba7a7629ffa690647c43dc07bea7f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.words()) # No. of words in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "c3ba66d4-3e26-40fc-94d3-211581fa4c5c",
    "_uuid": "2de0781902fe8662d73bda9381cab9e318cdebf8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.sents() # Returns a list of list of strings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "68725143-131d-4886-851c-f932c84588aa",
    "_uuid": "a17b7f140a86b0541acf2796b9146955acd64201"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.sents(fileids='ca01') # You can access a specific file with `fileids` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57340"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.sents())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "972700b2-ee0b-4038-ba23-573287d08fd0",
    "_uuid": "4e2791d121eb8162f2031df6c126bcaa40d812d7"
   },
   "source": [
    "\n",
    "**Fast Facts:**\n",
    "\n",
    "> The Brown Corpus of Standard American English was the first of the modern, computer readable, general corpora. It was compiled by W.N. Francis and H. Kucera, Brown University, Providence, RI. The corpus consists of one million words of American English texts printed in 1961.\n",
    "\n",
    "(Source: [University of Essex Corpus Linguistics site](  https://www1.essex.ac.uk/linguistics/external/clmt/w3c/corpus_ling/content/corpora/list/private/brown/brown.html))\n",
    "\n",
    ">  This corpus contains text from 500 sources, and the sources have been categorized by genre, such as news, editorial, and so on ... (for a complete list, see http://icame.uib.no/brown/bcm-los.html).\n",
    "\n",
    "![](http://)(Source: [NLTK book, Chapter 2.1.3](http://www.nltk.org/book/ch02.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1ecd0a80-5a12-444a-b9da-768ffef5133f",
    "_uuid": "abcb302ff5d44499870e06f7f86490cf077fea2b"
   },
   "source": [
    "The actual `brown` corpus data is **packaged as raw text files**.  And you can find their IDs with: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "9bb09e9a-574f-4c3d-97eb-2091ce989cab",
    "_uuid": "8c979bdf698b8f7975b1216083168de0317c8ac5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.fileids()) # 500 sources, each file is a source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "fca762b7-5403-446f-82ca-e91e4cbd51dd",
    "_uuid": "30e85f007a8812090f813d8212e43a834be3de29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ca01', 'ca02', 'ca03', 'ca04', 'ca05', 'ca06', 'ca07', 'ca08', 'ca09', 'ca10', 'ca11', 'ca12', 'ca13', 'ca14', 'ca15', 'ca16', 'ca17', 'ca18', 'ca19', 'ca20', 'ca21', 'ca22', 'ca23', 'ca24', 'ca25', 'ca26', 'ca27', 'ca28', 'ca29', 'ca30', 'ca31', 'ca32', 'ca33', 'ca34', 'ca35', 'ca36', 'ca37', 'ca38', 'ca39', 'ca40', 'ca41', 'ca42', 'ca43', 'ca44', 'cb01', 'cb02', 'cb03', 'cb04', 'cb05', 'cb06', 'cb07', 'cb08', 'cb09', 'cb10', 'cb11', 'cb12', 'cb13', 'cb14', 'cb15', 'cb16', 'cb17', 'cb18', 'cb19', 'cb20', 'cb21', 'cb22', 'cb23', 'cb24', 'cb25', 'cb26', 'cb27', 'cc01', 'cc02', 'cc03', 'cc04', 'cc05', 'cc06', 'cc07', 'cc08', 'cc09', 'cc10', 'cc11', 'cc12', 'cc13', 'cc14', 'cc15', 'cc16', 'cc17', 'cd01', 'cd02', 'cd03', 'cd04', 'cd05', 'cd06', 'cd07', 'cd08', 'cd09', 'cd10', 'cd11', 'cd12']\n"
     ]
    }
   ],
   "source": [
    "print(brown.fileids()[:100]) # First 100 sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "74143a0e-40e2-4450-9ab4-427686f6ac87",
    "_uuid": "b6572ea60d0142090d77a44fabc98ee29943a4bb"
   },
   "source": [
    "You can access the raw files with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "9adcf194-d863-4043-85d0-00743c2786c9",
    "_uuid": "3f73483a3d332c4be88f54115d48beaa6351e090"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembly/nn-hl session/nn-hl brought/vbd-hl much/ap-hl good/nn-hl \n",
      "The/at General/jj-tl Assembly/nn-tl ,/, which/wdt adjourns/vbz today/nr ,/, has/hvz performed/vbn in/in an/at atmosphere/nn of/in crisis/nn and/cc struggle/nn from/in the/at day/nn it/pps convened/vbd ./.\n",
      "It/pps was/bedz faced/vbn immediately/rb with/in a/at showdown/nn on/in the/at schools/nns ,/, an/at issue/nn which/wdt was/bedz met/vbn squarely/rb in/in conjunction/nn with/in the/at governor/nn with/in a/at decision/nn not/* to/to risk/vb abandoning/vbg public/nn education/nn ./.\n",
      "\n",
      "\n",
      "\tThere/ex followed/vbd the/at historic/jj appropriations/nns and/cc budget/nn fight/nn ,/, in/in which/wdt the/at General/jj-tl Assembly/nn-tl decided/vbd to/to tackle/vb executive/nn powers/nns ./.\n",
      "The/at final/jj decision/nn went/vbd to/in the/at executive/nn but/cc a/at way/nn has/hvz been/ben opened/vbn for/in strengthening/vbg budgeting/vbg procedures/nns and/cc to/to provide/vb legislators/nns information/nn they/ppss need/vb ./.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(brown.raw('cb01').strip()[:1000]) # First 1000 characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9c58248d-6e32-496f-843b-1a07e3d4afb8",
    "_uuid": "f293a3c986c26e82394bd58a68bf2a0843b40f80"
   },
   "source": [
    "<br>\n",
    "You will see that **each word comes with a slash and a label** and unlike normal text, we see that **punctuations are separated from the word that comes before it**, e.g. \n",
    "\n",
    "> The/at General/jj-tl Assembly/nn-tl ,/, which/wdt adjourns/vbz today/nr ,/, has/hvz performed/vbn in/in an/at atmosphere/nn of/in crisis/nn and/cc struggle/nn from/in the/at day/nn it/pps convened/vbd ./.\n",
    "\n",
    "<br>\n",
    "And we also see that the **each sentence is separated by a newline**:\n",
    "\n",
    "> There/ex followed/vbd the/at historic/jj appropriations/nns and/cc budget/nn fight/nn ,/, in/in which/wdt the/at General/jj-tl Assembly/nn-tl decided/vbd to/to tackle/vb executive/nn powers/nns ./.\n",
    "> \n",
    "> The/at final/jj decision/nn went/vbd to/in the/at executive/nn but/cc a/at way/nn has/hvz been/ben opened/vbn for/in strengthening/vbg budgeting/vbg procedures/nns and/cc to/to provide/vb legislators/nns information/nn they/ppss need/vb ./.\n",
    "\n",
    "<br>\n",
    "That brings us to the next point on **sentence tokenization** and **word tokenization**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "50b08633-6665-44e1-a989-e8b5fd3f11df",
    "_uuid": "2275a8deeace1a0080b4194d70a0e6f751026e2d"
   },
   "source": [
    "Tokenization\n",
    "====\n",
    "\n",
    "**Sentence tokenization** is the process of  *splitting up strings into “sentences”*\n",
    "\n",
    "**Word tokenization** is the process of  *splitting up “sentences” into “words”*\n",
    "\n",
    "Lets play around with some interesting texts,  the `singles.txt` from `webtext` corpus. <br>\n",
    "They were some  **singles ads** from  http://search.classifieds.news.com.au/\n",
    "\n",
    "First, downoad the data with `nltk.download()`:\n",
    "\n",
    "```python\n",
    "nltk.download('webtext')\n",
    "```\n",
    "\n",
    "Then you can import with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package webtext to\n",
      "[nltk_data]     /csehome/caelum02/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/webtext.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('webtext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "3636768b-77ad-4f0a-922d-1cedd2a8b11a",
    "_uuid": "0a0c9f31e40f7ef9f3172aad7b68dbe3d4cc1829"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import webtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "6f9fd322-e916-4b70-83c3-87078c66fb15",
    "_uuid": "f899b6ed306c96f7731f1acc51aaba4256ec59b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['firefox.txt',\n",
       " 'grail.txt',\n",
       " 'overheard.txt',\n",
       " 'pirates.txt',\n",
       " 'singles.txt',\n",
       " 'wine.txt']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webtext.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "7518ac61-756f-466b-8294-ca6056994415",
    "_uuid": "89a84224c596cf23b3498a7d5f3dbbf74428ef9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\t25 SEXY MALE, seeks attrac older single lady, for discreet encounters.\n",
      "1:\t35YO Security Guard, seeking lady in uniform for fun times.\n",
      "2:\t40 yo SINGLE DAD, sincere friendly DTE seeks r/ship with fem age open S/E\n",
      "3:\t44yo tall seeks working single mum or lady below 45 fship rship. Nat Open\n",
      "4:\t6.2 35 yr old OUTGOING M seeks fem 28-35 for o/door sports - w/e away\n",
      "5:\tA professional business male, late 40s, 6 feet tall, slim build, well groomed, great personality, home owner, interests include the arts travel and all things good, Ringwood area, is seeking a genuine female of similar age or older, in same area or surrounds, for a meaningful long term rship. Looking forward to hearing from you all.\n",
      "6:\tABLE young man seeks, sexy older women. Phone for fun ready to play\n",
      "7:\tAFFECTIONATE LADY Sought by generous guy, 40s, mutual fulfillment\n",
      "8:\tARE YOU ALONE or lost in a r/ship too, with no hope in sight? Maybe we could explore new beginnings together? Im 45 Slim/Med build, GSOH, high needs and looking for someone similar. You WONT be disappointed.\n",
      "9:\tAMIABLE 43 y.o. gentleman with European background, 170 cm, medium build, employed, never married, no children. Enjoys sports, music, cafes, beach &c. Seeks an honest, attractive lady with a European background, without children, who would like to get married and have chil dren in the future. 29-39 y.o. Prefer non-smoker and living in Adelaide.\n",
      "10:\tARE YOU A COPPER REDHEAD? I am 36 y.o. and looking for companionship/friendship. I enjoy the AFL, animals and dining out.\n"
     ]
    }
   ],
   "source": [
    "# Each line is one advertisement.\n",
    "for i, line in enumerate(webtext.raw('singles.txt').split('\\n')):\n",
    "    if i > 10: # Lets take a look at the first 10 ads.\n",
    "        break\n",
    "    print(str(i) + ':\\t' + line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "683d51eb-41e1-41bf-9bd2-3314d435f330",
    "_uuid": "c1086676ea2bd10932715c1dfc5ebca5f7765389"
   },
   "source": [
    "# Lets zoom in on candidate no. 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "7b46c556-583a-4af2-95d4-93d4d4b55bd2",
    "_uuid": "3f117b6fa1c9dc0150800cdbd632f4cee1c3fbde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARE YOU ALONE or lost in a r/ship too, with no hope in sight? Maybe we could explore new beginnings together? Im 45 Slim/Med build, GSOH, high needs and looking for someone similar. You WONT be disappointed.\n"
     ]
    }
   ],
   "source": [
    "single_no8 = webtext.raw('singles.txt').split('\\n')[8]\n",
    "print(single_no8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f227e306-1fc8-4d0a-bebc-55014599b588",
    "_uuid": "5d8daeb0c88fa79b07730d3d95d4f66382ebb0ef"
   },
   "source": [
    "# Sentence Tokenization\n",
    "<br>\n",
    "In NLTK, `sent_tokenize()` the default tokenizer function that you can use to split strings into \"*sentences*\". \n",
    "<br>\n",
    "\n",
    "It is using the [**Punkt algortihm** from Kiss and Strunk (2006)](http://www.mitpressjournals.org/doi/abs/10.1162/coli.2006.32.4.485)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "c49b219d-864f-4353-9535-648592f0d847",
    "_uuid": "a5430c319a9f9cc66f074405d24271fec49e77c5"
   },
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /csehome/caelum02/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "0666bda0-ebec-4f2b-ab92-2158b70e38a2",
    "_uuid": "1d1a02d8bb0892942a4a5059a46b7f8aa23b7e01"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ARE YOU ALONE or lost in a r/ship too, with no hope in sight?',\n",
       " 'Maybe we could explore new beginnings together?',\n",
       " 'Im 45 Slim/Med build, GSOH, high needs and looking for someone similar.',\n",
       " 'You WONT be disappointed.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(single_no8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "17ab6433-7166-4e87-837a-74cde568f98f",
    "_uuid": "f1198990be58fd7d81cb6516651f98a2716824c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ARE', 'YOU', 'ALONE', 'or', 'lost', 'in', 'a', 'r/ship', 'too', ',', 'with', 'no', 'hope', 'in', 'sight', '?']\n",
      "['Maybe', 'we', 'could', 'explore', 'new', 'beginnings', 'together', '?']\n",
      "['Im', '45', 'Slim/Med', 'build', ',', 'GSOH', ',', 'high', 'needs', 'and', 'looking', 'for', 'someone', 'similar', '.']\n",
      "['You', 'WONT', 'be', 'disappointed', '.']\n"
     ]
    }
   ],
   "source": [
    "for sent in sent_tokenize(single_no8):\n",
    "    print(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = '''\n",
    " 지난 5월8일 제주에서 경기 도중 갑자기 쓰러진 신영록 선수를 김장열 트레이너팀장은 다급하게 심폐소생술(CPR)을 실시했다. 쓰러지고 4-5분후 앰\n",
    "뷸런스에 태워졌고 약 10분후 제주한라병원 응급의료센터에 도착했다. 응급의학과 양석진 과장은 심전도를 연결해 모니터를 확인하여 즉각 제세동과 기도 삽관을 실시했다. 이 후 중환자실로 옮겨 저체온치료에 들어갔다.\n",
    " 신영록 선수는 생명의 고리 5단계의 신속하고 적절한 연결로 지금 그라운드 복귀를 꿈꾸며 킥연습을 하고 있다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n 지난 5월8일 제주에서 경기 도중 갑자기 쓰러진 신영록 선수를 김장열 트레이너팀장은 다급하게 심폐소생술(CPR)을 실시했다.',\n",
       " '쓰러지고 4-5분후 앰\\n뷸런스에 태워졌고 약 10분후 제주한라병원 응급의료센터에 도착했다.',\n",
       " '응급의학과 양석진 과장은 심전도를 연결해 모니터를 확인하여 즉각 제세동과 기도 삽관을 실시했다.',\n",
       " '이 후 중환자실로 옮겨 저체온치료에 들어갔다.',\n",
       " '신영록 선수는 생명의 고리 5단계의 신속하고 적절한 연결로 지금 그라운드 복귀를 꿈꾸며 킥연습을 하고 있다.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['지난',\n",
       " '5월8일',\n",
       " '제주에서',\n",
       " '경기',\n",
       " '도중',\n",
       " '갑자기',\n",
       " '쓰러진',\n",
       " '신영록',\n",
       " '선수를',\n",
       " '김장열',\n",
       " '트레이너팀장은',\n",
       " '다급하게',\n",
       " '심폐소생술',\n",
       " '(',\n",
       " 'CPR',\n",
       " ')',\n",
       " '을',\n",
       " '실시했다',\n",
       " '.',\n",
       " '쓰러지고',\n",
       " '4-5분후',\n",
       " '앰',\n",
       " '뷸런스에',\n",
       " '태워졌고',\n",
       " '약',\n",
       " '10분후',\n",
       " '제주한라병원',\n",
       " '응급의료센터에',\n",
       " '도착했다',\n",
       " '.',\n",
       " '응급의학과',\n",
       " '양석진',\n",
       " '과장은',\n",
       " '심전도를',\n",
       " '연',\n",
       " '결해',\n",
       " '모니터를',\n",
       " '확인하여',\n",
       " '즉각',\n",
       " '제세동과',\n",
       " '기도',\n",
       " '삽관을',\n",
       " '실시했다',\n",
       " '.',\n",
       " '이',\n",
       " '후',\n",
       " '중환자실로',\n",
       " '옮겨',\n",
       " '저체온치료에',\n",
       " '들어갔다',\n",
       " '.',\n",
       " '신영록',\n",
       " '선수는',\n",
       " '생명의',\n",
       " '고리',\n",
       " '5단계의',\n",
       " '신속하고',\n",
       " '적절한',\n",
       " '연결로',\n",
       " '지금',\n",
       " '그라운드',\n",
       " '복귀를',\n",
       " '꿈꾸며',\n",
       " '킥연습을',\n",
       " '하고',\n",
       " '있다',\n",
       " '.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e11cdead-66ae-4e25-9e68-9130297e0758",
    "_uuid": "0ee17d9e697179e1db2bd8ef95cae55f3b5d64b3"
   },
   "source": [
    "# Lowercasing\n",
    "\n",
    "The CAPS in the texts are RATHER irritating although we KNOW the guy is trying to EMPHASIZE on something ;P\n",
    "\n",
    "We can simply **lowercase them after we do `sent_tokenize()` and `word_tokenize()`**. <br>\n",
    "The tokenizers uses the capitalization as cues to know when to split so removing them before the calling the functions would be sub-optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_cell_guid": "665cff0c-7140-444a-8ea9-ce3e20299464",
    "_uuid": "197cae1396cc555d02eb64676471a564e1e7f35a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ARE YOU ALONE or lost in a r/ship too, with no hope in sight?',\n",
       " 'Maybe we could explore new beginnings together?',\n",
       " 'Im 45 Slim/Med build, GSOH, high needs and looking for someone similar.',\n",
       " 'You WONT be disappointed.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(single_no8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_cell_guid": "42c2c2d9-cb6b-41d4-ac89-46440b13e94c",
    "_uuid": "9f54f59d10281ff66ec36648cff7eda1f13f5ba2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['are', 'you', 'alone', 'or', 'lost', 'in', 'a', 'r/ship', 'too', ',', 'with', 'no', 'hope', 'in', 'sight', '?']\n",
      "['maybe', 'we', 'could', 'explore', 'new', 'beginnings', 'together', '?']\n",
      "['im', '45', 'slim/med', 'build', ',', 'gsoh', ',', 'high', 'needs', 'and', 'looking', 'for', 'someone', 'similar', '.']\n",
      "['you', 'wont', 'be', 'disappointed', '.']\n"
     ]
    }
   ],
   "source": [
    "for sent in sent_tokenize(single_no8):\n",
    "    # It's a little in efficient to loop through each word,\n",
    "    # after but sometimes it helps to get better tokens.\n",
    "    #print([word.lower() for word in word_tokenize(sent)])\n",
    "    # Alternatively:\n",
    "    print(list(map(str.lower, word_tokenize(sent))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_cell_guid": "08dea492-753c-4dba-8ac1-90c8dce48aef",
    "_uuid": "b90631ac206dc4a6495729b06bcd1fc6415134d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ARE', 'YOU', 'ALONE', 'or', 'lost', 'in', 'a', 'r/ship', 'too', ',', 'with', 'no', 'hope', 'in', 'sight', '?', 'Maybe', 'we', 'could', 'explore', 'new', 'beginnings', 'together', '?', 'Im', '45', 'Slim/Med', 'build', ',', 'GSOH', ',', 'high', 'needs', 'and', 'looking', 'for', 'someone', 'similar', '.', 'You', 'WONT', 'be', 'disappointed', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(single_no8))  # Treats the whole line as one document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "eebfbac8-8b29-46ad-b2e2-e5badc926e4b",
    "_uuid": "7e5a3cc059f151e0ce4f975e56bc98205c03e90f"
   },
   "source": [
    "# Tangential Note\n",
    "\n",
    "Punkt is a statistical model so it applies the knowledge it has learnt from previous data. <br>\n",
    "Generally, it **works for most cases on well-formed texts** but if your data is  different e.g. user-generated noisy texts, you might have to retrain a new model. \n",
    "![](http://)\n",
    "E.g. if we look at candidate no. 9, we see that it's splitting on `y.o.` (its thinking that its the end of the sentnence) and not splitting on `&c.` (its thinking that its an abbreviation, e.g. `Mr.`, `Inc.`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_cell_guid": "e3843142-7ff7-4e6d-8430-191ee8b4ce47",
    "_uuid": "ef345b667df7113cba651ae8edbeb0e213ee6809"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AMIABLE 43 y.o.',\n",
       " 'gentleman with European background, 170 cm, medium build, employed, never married, no children.',\n",
       " 'Enjoys sports, music, cafes, beach &c. Seeks an honest, attractive lady with a European background, without children, who would like to get married and have chil dren in the future.',\n",
       " '29-39 y.o.',\n",
       " 'Prefer non-smoker and living in Adelaide.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_no9 = webtext.raw('singles.txt').split('\\n')[9]\n",
    "sent_tokenize(single_no9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3d1a2d29-ab44-44f7-803a-ac3561368f09",
    "_uuid": "df75cb932724a29599f66c6229ba3f324a690181"
   },
   "source": [
    "Stopwords\n",
    "====\n",
    "\n",
    "**Stopwords** are non-content words that primarily has only grammatical function\n",
    "\n",
    "In NLTK, you can access them as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /csehome/caelum02/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_cell_guid": "450cc60b-4e90-491f-9dd3-92ae22fd979a",
    "_uuid": "501da9982fc07b1deaae173ed816caa01ac2dd79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_en = stopwords.words('english')\n",
    "print(stopwords_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e02fc6ff-713d-4090-b7e2-3132c9415549",
    "_uuid": "e613de7ef925754b74334b79788acf4648b9be0b"
   },
   "source": [
    "# Often we want to remove stopwords when we want to keep the \"gist\" of the document/sentence.\n",
    "\n",
    "For instance, lets go back to the our `single_no8`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_cell_guid": "17a72a56-a94e-4d0b-af61-370d5f7b3adb",
    "_uuid": "ceef2a1d450726b705b22698b5b35635c942be38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['are', 'you', 'alone', 'or', 'lost', 'in', 'a', 'r/ship', 'too', ',', 'with', 'no', 'hope', 'in', 'sight', '?', 'maybe', 'we', 'could', 'explore', 'new', 'beginnings', 'together', '?', 'im', '45', 'slim/med', 'build', ',', 'gsoh', ',', 'high', 'needs', 'and', 'looking', 'for', 'someone', 'similar', '.', 'you', 'wont', 'be', 'disappointed', '.']\n"
     ]
    }
   ],
   "source": [
    "# Treat the multiple sentences as one document (no need to sent_tokenize)\n",
    "# Tokenize and lowercase\n",
    "single_no8_tokenized_lowered = list(map(str.lower, word_tokenize(single_no8)))\n",
    "print(single_no8_tokenized_lowered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ea203327-c900-4d54-a1cd-fb3d94a9f021",
    "_uuid": "7232e7aacc09962b8b010f6463ff14566471f541"
   },
   "source": [
    "# Let's try to remove the stopwords using the English stopwords list in NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_cell_guid": "b97fa9b8-6366-40b6-9b0f-05a15fc3b93f",
    "_uuid": "3b691c536a5487f33b92eceb94b34b8d12cac22f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alone', 'lost', 'r/ship', ',', 'hope', 'sight', '?', 'maybe', 'could', 'explore', 'new', 'beginnings', 'together', '?', 'im', '45', 'slim/med', 'build', ',', 'gsoh', ',', 'high', 'needs', 'looking', 'someone', 'similar', '.', 'wont', 'disappointed', '.']\n"
     ]
    }
   ],
   "source": [
    "stopwords_en = set(stopwords.words('english')) # Set checking is faster in Python than list.\n",
    "\n",
    "# List comprehension.\n",
    "print([word for word in single_no8_tokenized_lowered if word not in stopwords_en])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a6ebff46-17a3-47d0-bf1f-47461ee484a4",
    "_uuid": "d99001efcddcd7a080d76f6064ce309e41542f86"
   },
   "source": [
    "# Often, we want to remove the punctuations from the documents too.\n",
    "\n",
    "Since Python comes with \"batteries included\", we have string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_cell_guid": "fd82dfe3-2221-4c0a-9d0d-1b14a5349b91",
    "_uuid": "991965258d7aeffa5f55420fdf4e6f4a83dbfd9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From string.punctuation: <class 'str'> !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "# It's a string so we have to them into a set type\n",
    "print('From string.punctuation:', type(punctuation), punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c83a6e53-e7f4-43c9-a0b7-522ca1e75e55",
    "_uuid": "78edf4a03bd59a7753ba5a0d9a238df439711516"
   },
   "source": [
    "# Combining the punctuation with the stopwords from NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_cell_guid": "ffafed92-9c4c-4fc1-aa7b-89645edf4b8c",
    "_uuid": "97c96c2b0df9d8cd1dc6c6cf6619ed7f09c3072c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'!', \"wouldn't\", 'all', '~', 'by', 'didn', 'nor', 'while', \"she's\", 'they', 'during', \"hadn't\", 'until', \"weren't\", 'hadn', \"didn't\", '@', 'these', 'itself', 'down', 'other', 're', \"doesn't\", 'into', '{', \"mustn't\", 'when', \"shouldn't\", 'did', 'been', '\"', 'was', \"that'll\", 'theirs', \"haven't\", 'how', \"wasn't\", 'myself', 'him', '-', 'wouldn', 'have', ';', 'i', 'too', 'yourself', 'had', 'o', 'me', 'most', 'only', 'and', 'again', 'ma', 'weren', 'doing', ')', 'through', 'you', 'm', \"couldn't\", \"isn't\", 'few', '_', ']', 'at', 'that', 'both', 'do', 'why', 'to', 'we', '<', 'for', 'them', \"won't\", '?', 'its', 'below', 'out', 'she', 'ourselves', '=', 'because', 'in', 'ain', 'will', 'this', '*', '&', 'be', 'does', 'our', 'themselves', \"should've\", 'very', \"don't\", 'his', \"needn't\", 'whom', 'd', '\\\\', 's', 'if', 'he', 'but', \"you've\", 'being', 'who', 'which', 'her', 'shouldn', '#', 'each', 'not', \"you'll\", 'am', 'yours', 'just', \"you'd\", 'himself', 'my', 'mustn', 'once', 'wasn', ',', 'the', 'above', 'what', 'a', 'over', 'off', 'having', 'hasn', 'more', 'before', 'so', 'here', 'an', 'now', \"aren't\", 'is', 'under', 'has', \"hasn't\", 'after', ':', 'there', 't', '>', 'any', 'where', 've', 'aren', '%', 'between', 'should', 'yourselves', 'same', '|', 'won', 'than', '`', 'your', 'needn', 'further', 'it', 'their', 'from', 'those', 'can', 'hers', \"mightn't\", 'such', 'or', \"shan't\", 'couldn', '}', '(', 'against', \"'\", '.', 'don', 'll', 'as', '$', 'shan', 'are', '+', 'were', '/', 'on', 'doesn', 'with', 'haven', \"you're\", 'then', 'ours', 'some', 'isn', '^', 'mightn', '[', 'of', 'no', 'herself', 'own', 'y', 'about', 'up', \"it's\"}\n"
     ]
    }
   ],
   "source": [
    "stopwords_en_withpunct = stopwords_en.union(set(punctuation))\n",
    "print(stopwords_en_withpunct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "230db9d8-bb06-4e48-aad9-4c4609448c4a",
    "_uuid": "e6b6acafbca2e1c8f88fbda2313da9d9b4961cf4"
   },
   "source": [
    "# Removing stopwords with punctuations from Single no. 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_cell_guid": "8cc6b68c-29b1-48b2-baa1-bd3f9bc4e21f",
    "_uuid": "c37dbd0c8a7658dd20b2266276baf0e162204576",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alone', 'lost', 'r/ship', 'hope', 'sight', 'maybe', 'could', 'explore', 'new', 'beginnings', 'together', 'im', '45', 'slim/med', 'build', 'gsoh', 'high', 'needs', 'looking', 'someone', 'similar', 'wont', 'disappointed']\n"
     ]
    }
   ],
   "source": [
    "print([word for word in single_no8_tokenized_lowered if word not in stopwords_en_withpunct])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "31f5c735-c260-4a4d-870e-721ccca5cb8b",
    "_uuid": "027cd197d81df7b34048c520a84270c74781a6f0"
   },
   "source": [
    "# Using a stronger/longer list of stopwords\n",
    "\n",
    "From the previous output, we have still dangly model verbs (i.e. 'could', 'wont', etc.).\n",
    "\n",
    "We can combine the stopwords we have in NLTK with other stopwords list we find online.\n",
    "\n",
    "Personally, I like to use `stopword-json` because it has stopwrds in 50 languages =) <br>\n",
    "https://github.com/6/stopwords-json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_cell_guid": "adf1ed39-3116-46f0-92c0-6dfddcd904a0",
    "_uuid": "376d2a04068b557bc5080353a998aa52c199dccb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With combined stopwords:\n",
      "['lost', 'r/ship', 'hope', 'sight', 'explore', 'beginnings', 'im', '45', 'slim/med', 'build', 'gsoh', 'high', 'similar', 'wont', 'disappointed']\n"
     ]
    }
   ],
   "source": [
    "# Stopwords from stopwords-json\n",
    "stopwords_json = {\"en\":[\"a\",\"a's\",\"able\",\"about\",\"above\",\"according\",\"accordingly\",\"across\",\"actually\",\"after\",\"afterwards\",\"again\",\"against\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\"available\",\"away\",\"awfully\",\"b\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"c\",\"c'mon\",\"c's\",\"came\",\"can\",\"can't\",\"cannot\",\"cant\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clearly\",\"co\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\"couldn't\",\"course\",\"currently\",\"d\",\"definitely\",\"described\",\"despite\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"done\",\"down\",\"downwards\",\"during\",\"e\",\"each\",\"edu\",\"eg\",\"eight\",\"either\",\"else\",\"elsewhere\",\"enough\",\"entirely\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"f\",\"far\",\"few\",\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"four\",\"from\",\"further\",\"furthermore\",\"g\",\"get\",\"gets\",\"getting\",\"given\",\"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\"gotten\",\"greetings\",\"h\",\"had\",\"hadn't\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he's\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\"hopefully\",\"how\",\"howbeit\",\"however\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"ie\",\"if\",\"ignored\",\"immediate\",\"in\",\"inasmuch\",\"inc\",\"indeed\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"instead\",\"into\",\"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"it's\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\"know\",\"known\",\"knows\",\"l\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"like\",\"liked\",\"likely\",\"little\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\"mainly\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"meanwhile\",\"merely\",\"might\",\"more\",\"moreover\",\"most\",\"mostly\",\"much\",\"must\",\"my\",\"myself\",\"n\",\"name\",\"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"no\",\"nobody\",\"non\",\"none\",\"noone\",\"nor\",\"normally\",\"not\",\"nothing\",\"novel\",\"now\",\"nowhere\",\"o\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"or\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"own\",\"p\",\"particular\",\"particularly\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"possible\",\"presumably\",\"probably\",\"provides\",\"q\",\"que\",\"quite\",\"qv\",\"r\",\"rather\",\"rd\",\"re\",\"really\",\"reasonably\",\"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\"s\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"she\",\"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\"sub\",\"such\",\"sup\",\"sure\",\"t\",\"t's\",\"take\",\"taken\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that's\",\"thats\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there's\",\"thereafter\",\"thereby\",\"therefore\",\"therein\",\"theres\",\"thereupon\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"think\",\"third\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"twice\",\"two\",\"u\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"us\",\"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"uucp\",\"v\",\"value\",\"various\",\"very\",\"via\",\"viz\",\"vs\",\"w\",\"want\",\"wants\",\"was\",\"wasn't\",\"way\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"welcome\",\"well\",\"went\",\"were\",\"weren't\",\"what\",\"what's\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"who's\",\"whoever\",\"whole\",\"whom\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"won't\",\"wonder\",\"would\",\"wouldn't\",\"x\",\"y\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"z\",\"zero\"]}\n",
    "stopwords_json_en = set(stopwords_json['en'])\n",
    "stopwords_nltk_en = set(stopwords.words('english'))\n",
    "stopwords_punct = set(punctuation)\n",
    "# Combine the stopwords. Its a lot longer so I'm not printing it out...\n",
    "stoplist_combined = set.union(stopwords_json_en, stopwords_nltk_en, stopwords_punct)\n",
    "\n",
    "# Remove the stopwords from `single_no8`.\n",
    "print('With combined stopwords:')\n",
    "print([word for word in single_no8_tokenized_lowered if word not in stoplist_combined])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## punctuation 없애는 간단한 방법 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent= '''\n",
    "#문장 1# (지문) $3\n",
    "‘부석사 설화’는 신라시대 고승 의상이 당나라에서; [공부하던] 중 알게 된, 선묘(善妙)라는 <여인과의>\n",
    "못다한 사랑 이야기를 토대로 용과 (여인의) 애절한 사랑을 그린 것이다!\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#문장 1# (지문) $3\n",
      "‘부석사 설화’는 신라시대 고승 의상이 당나라에서; [공부하던] 중 알게 된, 선묘(善妙)라는 <여인과의>\n",
      "못다한 사랑 이야기를 토대로 용과 (여인의) 애절한 사랑을 그린 것이다!\n",
      "\n",
      "\n",
      "문장 1 지문 3\n",
      "‘부석사 설화’는 신라시대 고승 의상이 당나라에서 공부하던 중 알게 된 선묘善妙라는 여인과의\n",
      "못다한 사랑 이야기를 토대로 용과 여인의 애절한 사랑을 그린 것이다\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "out = sent.translate(str.maketrans('', '', string.punctuation))\n",
    "print(sent)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한국어 신문기사 Cleaning을 위한 puncList만들기\n",
    "- 신문기사에서 쓰이는 한국어 특유의 puncuation list를 만들어 cleaning에 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. set으로 지워야 할 표현들을 유지 관리\n",
    "#removal_ko = {'‘','’', '◇', '“', '‘', '”', '’'} #set\n",
    "\n",
    "#removal_ko_with_punct = removal_ko.union(set(string.punctuation)) #set\n",
    "#removal_list = repr(removal_ko_with_punct) #back to string\n",
    "#print(removal_list)\n",
    "\n",
    "#2.  or just string enough\n",
    "removal_list =  \"‘, ’, ◇, ‘, ”,  ’, ·, “\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    " ◇재정일자리 56만3천개 마련. 여야는 직업훈련을 전제로 청·장\n",
    "년 실업자에게 수당을 줘서 구직을 유도하는 제도를 새로 도입하는데 1천529억원의 예산을 신규 편성했다\n",
    "“올해의 역사, ‘전라도(全羅道)’가 쓴다”\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "재정일자리56만3천개마련.여야는직업훈련을전제로청장\n",
      "년실업자에게수당을줘서구직을유도하는제도를새로도입하는데1천529억원의예산을신규편성했다\n",
      "올해의역사전라도(全羅道)가쓴다\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out2 = text.translate(str.maketrans('', '', removal_list))\n",
    "print(out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  재정일자리 56만3천개 마련. 여야는 직업훈련을 전제로 청 장\n",
      "년 실업자에게 수당을 줘서 구직을 유도하는 제도를 새로 도입하는데 1천529억원의 예산을 신규 편성했다\n",
      " 올해의 역사   전라도(全羅道) 가 쓴다 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "out2 = text.translate(str.maketrans(removal_list, ' '*len(removal_list))) #map punctuation to space\n",
    "print(out2)\n",
    "                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### maketrans()\n",
    "\n",
    "maketrans() function is used to construct the transition table i.e specify the list of characters that need to be replaced in the whole string or the characters that need to be deleted from the string\n",
    "\n",
    "Syntax : maketrans(str1, str2, str3)\n",
    "\n",
    "Parameters :\n",
    "str1 : Specifies the list of characters that need to be replaced.\n",
    "str2 : Specifies the list of characters with which the characters need to be replaced.\n",
    "str3 : Specifies the list of characters that needs to be deleted.\n",
    "\n",
    "Returns : Returns the translation table which specifies the conversions that can be used by translate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate using maketrans()\n",
    "\n",
    "To translate the characters in the string translate() is used to make the translations. This function uses the translation mapping specified using the maketrans().\n",
    "\n",
    "Syntax : translate(table, delstr)\n",
    "\n",
    "Parameters :\n",
    "table : Translate mapping specified to perform translations.\n",
    "delstr : The delete string can be specified as optional argument is not mentioned in table.\n",
    "\n",
    "Returns : Returns the argument string after performing the translations using the translation table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The string before translating is : weeksyourweeks\n",
      "The string after translating is : geeksforgeeks\n"
     ]
    }
   ],
   "source": [
    "#https://www.geeksforgeeks.org/python-maketrans-translate-functions/\n",
    "    # Python3 code to demonstrate  \n",
    "# translations using  \n",
    "# maketrans() and translate() \n",
    "  \n",
    "# specify to translate chars \n",
    "str1 = \"wy\"\n",
    "  \n",
    "# specify to replace with \n",
    "str2 = \"gf\"\n",
    "  \n",
    "# delete chars \n",
    "str3 = \"u\"\n",
    "  \n",
    "# target string  \n",
    "trg = \"weeksyourweeks\"\n",
    "  \n",
    "# using maketrans() to  \n",
    "# construct translate \n",
    "# table \n",
    "table = trg.maketrans(str1, str2, str3) \n",
    "  \n",
    "# Printing original string  \n",
    "print (\"The string before translating is : \", end =\"\") \n",
    "print (trg) \n",
    "  \n",
    "# using translate() to make translations. \n",
    "print (\"The string after translating is : \", end =\"\") \n",
    "print (trg.translate(table)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate without maketrans()\n",
    "\n",
    "Translation can also be achieved by specifying the translation dictionary and passing as an object which acts as a mapping. In this case, there is no need of maketrans() to perform translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The string before translating is : weeksyourweeks\n",
      "The string after translating is : geeksforgeeks\n"
     ]
    }
   ],
   "source": [
    "# Python3 code to demonstrate  \n",
    "# translations without \n",
    "# maketrans()  \n",
    "  \n",
    "# specifying the mapping  \n",
    "# using ASCII  \n",
    "table = { 119 : 103, 121 : 102, 117 : None } \n",
    "  \n",
    "# target string  \n",
    "trg = \"weeksyourweeks\"\n",
    "  \n",
    "# Printing original string  \n",
    "print (\"The string before translating is : \", end =\"\") \n",
    "print (trg) \n",
    "  \n",
    "# using translate() to make translations. \n",
    "print (\"The string after translating is : \", end =\"\") \n",
    "print (trg.translate(table)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "273f7eb4-77ff-4cb9-abf0-4254122fa2b7",
    "_uuid": "84c1307d0bbc465925ef9932eddcbcdfd95a17ca"
   },
   "source": [
    "# Stemming and Lemmatization\n",
    "\n",
    "Often we want to map the different forms of the same word to the same root word, e.g. \"walks\", \"walking\", \"walked\" should all be the same as \"walk\".\n",
    "\n",
    "The stemming and lemmatization process are hand-written regex rules written find the root word.\n",
    "\n",
    " - **Stemming**: Trying to shorten a word with simple regex rules\n",
    "\n",
    " - **Lemmatization**: Trying to find the root word with linguistics rules (with the use of regexes)\n",
    "\n",
    "(See also: [Stemmers vs Lemmatizers](https://stackoverflow.com/q/17317418/610569) question on StackOverflow)\n",
    "\n",
    "There are various stemmers and one lemmatizer in NLTK, the most common being:\n",
    "\n",
    " - **Porter Stemmer** from [Porter (1980)](https://tartarus.org/martin/PorterStemmer/index.html)\n",
    " - **Wordnet Lemmatizer** (port of the Morphy: https://wordnet.princeton.edu/man/morphy.7WN.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_cell_guid": "f41dab63-6133-4a51-9eb6-f87414b30c30",
    "_uuid": "3dd75f548e7363aa69b9da126a886f7299e052be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walk\n",
      "walk\n",
      "walk\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "for word in ['walking', 'walks', 'walked']:\n",
    "    print(porter.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_cell_guid": "edf939e2-bc38-4c50-92db-fd8d958ad17d",
    "_uuid": "31f49462e63529ac6d25b16389ef8494343b4de4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walking\n",
      "walk\n",
      "walked\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "for word in ['walking', 'walks', 'walked']:\n",
    "    print(wnl.lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fe9ffb81-f53a-4c58-97b6-86e0d79c51ca",
    "_uuid": "d2fa9f729a413a1417461683aa667f55a112e4e2"
   },
   "source": [
    "# Gotcha! The lemmatizer is actually pretty complicated, it needs Parts of Speech (POS) tags.\n",
    "\n",
    "\n",
    "We won't cover what's POS today so I'll just show you how to \"whip\" the lemmatizer to do what you need.\n",
    "\n",
    "By default, the WordNetLemmatizer.lemmatize() function will assume that the word is a Noun if there's no explict POS tag in the input.\n",
    "\n",
    "First you need the pos_tag function to tag a sentence and using the tag convert it into WordNet tagsets and then put it through to the WordNetLemmatizer.\n",
    "\n",
    "**Note:** Lemmatization won't really work on single words alone without context or knowledge of its POS tag (i.e. we need to know whether the word is a noun, verb, adjective, adverb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "_cell_guid": "0fa94379-f1b2-4228-b30c-001d2895b993",
    "_uuid": "0813197200488bb07b8e23f5281ae324b323e981"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('He', 'PRP'), ('is', 'VBZ'), ('walking', 'VBG'), ('to', 'TO'), ('school', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def penn2morphy(penntag):\n",
    "    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n",
    "    morphy_tag = {'NN':'n', 'JJ':'a',\n",
    "                  'VB':'v', 'RB':'r'}\n",
    "    try:\n",
    "        return morphy_tag[penntag[:2]]\n",
    "    except:\n",
    "        return 'n' # if mapping isn't found, fall back to Noun.\n",
    "    \n",
    "# `pos_tag` takes the tokenized sentence as input, i.e. list of string,\n",
    "# and returns a tuple of (word, tg), i.e. list of tuples of strings\n",
    "# so we need to get the tag from the 2nd element.\n",
    "\n",
    "walking_tagged = pos_tag(word_tokenize('He is walking to school'))\n",
    "print(walking_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_cell_guid": "6a826e5e-1c16-46ad-ac52-916a2f4e5c41",
    "_uuid": "e59549b59e571e2368d7c08c80250a854259d1d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he', 'be', 'walk', 'to', 'school']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) for word, tag in walking_tagged]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "46b8011c-beba-4e69-8305-f5b98659e679",
    "_uuid": "034d83608a867c28c06b86c9dc129b2f88faf326"
   },
   "source": [
    "# Now, lets create a new lemmatization function for sentences given what we learnt above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_cell_guid": "6fe6901c-88ca-438f-9a9a-b1f28128ffc4",
    "_uuid": "4f7803ff5cbc3f6c0dc97ce9c353a0bec39a5696"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he', 'be', 'walk', 'to', 'school']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def penn2morphy(penntag):\n",
    "    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n",
    "    morphy_tag = {'NN':'n', 'JJ':'a',\n",
    "                  'VB':'v', 'RB':'r'}\n",
    "    try:\n",
    "        return morphy_tag[penntag[:2]]\n",
    "    except:\n",
    "        return 'n' \n",
    "    \n",
    "def lemmatize_sent(text): \n",
    "    # Text input is string, returns lowercased strings.\n",
    "    return [wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) \n",
    "            for word, tag in pos_tag(word_tokenize(text))]\n",
    "\n",
    "lemmatize_sent('He is walking to school')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "346bab96-cf6c-4a91-b27d-bfced577ea15",
    "_uuid": "6ad66c3cabdf30287f2f6b0ea5ccbfdcd37cf927"
   },
   "source": [
    "# Lets try the `lemmatize_sent()` and remove stopwords from Single no. 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_cell_guid": "926f7ec4-12c7-4d8d-8f2b-ef66b0b3a5a4",
    "_uuid": "e591e6f136501cee25961fb1df0e68f1eb676b45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Single no. 8:\n",
      "ARE YOU ALONE or lost in a r/ship too, with no hope in sight? Maybe we could explore new beginnings together? Im 45 Slim/Med build, GSOH, high needs and looking for someone similar. You WONT be disappointed. \n",
      "\n",
      "Lemmatized and removed stopwords:\n",
      "['lose', 'r/ship', 'hope', 'sight', 'explore', 'beginning', 'im', 'slim/med', 'build', 'gsoh', 'high', 'similar', 'wont', 'disappoint']\n"
     ]
    }
   ],
   "source": [
    "print('Original Single no. 8:')\n",
    "print(single_no8, '\\n')\n",
    "print('Lemmatized and removed stopwords:')\n",
    "print([word for word in lemmatize_sent(single_no8) \n",
    "       if word not in stoplist_combined\n",
    "       and not word.isdigit() ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9c245e23-b526-4d86-9082-912c1aa87255",
    "_uuid": "56629a24758169967e39e8a93576195fb7c27ee8"
   },
   "source": [
    "# Combining what we know about removing stopwords and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "_cell_guid": "358ca69f-e7b2-4b93-ad09-5cf1b0a7143a",
    "_uuid": "3145c9a5568eb2ed075eddf5a617d20b2286f012"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Input: str, i.e. document/sentence\n",
    "    # Output: list(str) , i.e. list of lemmas\n",
    "    return [word for word in lemmatize_sent(text) \n",
    "            if word not in stoplist_combined\n",
    "            and not word.isdigit()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "51ffeed8-ea00-479e-bc92-422f4e0bba0e",
    "_uuid": "099da61cd361f9128632758422960862742b917f"
   },
   "source": [
    "# Tangential Note on Lemmatization\n",
    "\n",
    "In English, a root word / lemma can manifest in different forms. \n",
    "\n",
    "| <img src=\"https://media1.giphy.com/media/xHHsXH7WJsWK4/giphy.gif\" align=\"left\" height=\"200\" width=\"200\"> | <img src=\"https://media1.giphy.com/media/xHHsXH7WJsWK4/giphy.gif\" align=\"left\" height=\"200\" width=\"200\"><img src=\"https://media1.giphy.com/media/xHHsXH7WJsWK4/giphy.gif\" align=\"left\" height=\"200\" width=\"200\"> |\n",
    "|:-------------:|:-------------:|\n",
    "| 1 cat  | 2 cats  |\n",
    "| 1 cat  | 2 cats  |\n",
    "\n",
    "For instance, we use “cat” to refer to a single “cat” and we attach an “-s”  suffix to refer to more than one cat, e.g. “two cats”. \n",
    "\n",
    "| <img src=\"https://68.media.tumblr.com/b0755247c8f32f79413d34b0410ccff1/tumblr_o3q8wlGi9v1u9ia8fo1_500.gif\" align=\"left\" height=\"200\" width=\"400\"> | \n",
    "|:-------------:| \n",
    "| cats walk / cats (are) walking | \n",
    "\n",
    "<!-- | <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/fb/ModelsCatwalk.jpg/440px-ModelsCatwalk.jpg\" align=\"left\" height=\"200\" width=\"400\"> | \n",
    "|:-------------:| \n",
    "| cat walk(s) / catwalk(s) |  --> \n",
    "\n",
    "\n",
    "Another example, the word “walk” has different forms, e.g. “walking” and “walked” indicate the time and/or progress of the walking motion. <!-- ~~Additionally, “walk” can also refer to the act of walking which is different from the walking motion, e.g. \"John went for a walk\" (act of walking) vs \"John wanted to walk to the park\" (the walking action/motion).~~ --> We refer to these root words as ***word types*** (e.g. “cat” and “walk”) and their different forms as ***word tokens*** (e.g. “cats”, “walk”, “walking”, “walked”, “walks”). \n",
    "\n",
    "Linguists further distinguish words between their lemmas or word families. A lemma refers to the canonical root word used as a dictionary entry. A word family refers to a group of lemmas which are derived from a single root word. Even though \"walkable\" would be a separate entry in a dictionary from \"walk\", \"walkable\" can be grouped under the word family of \"walk\" together with \"walking, walked, walks\".\n",
    "\n",
    "The distinction is subtle yet linguists go into great length to argue for what counts as a type, token, lemmas or word family. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.7.9"
=======
   "version": "3.8.3"
>>>>>>> b6ea620bb63e6b4d1ceb3647760682bde11dc38f
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
